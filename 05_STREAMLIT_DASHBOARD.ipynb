{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344248ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# PAGE CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Halal/Haram Food Classifier\",\n",
    "    page_icon=\"üçî\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM CSS\n",
    "# ============================================================================\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        font-size: 3rem;\n",
    "        color: #2E86AB;\n",
    "        text-align: center;\n",
    "        font-weight: bold;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    .sub-header {\n",
    "        font-size: 1.2rem;\n",
    "        color: #666;\n",
    "        text-align: center;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .halal-result {\n",
    "        background: linear-gradient(135deg, #06A77D 0%, #05d69e 100%);\n",
    "        padding: 2rem;\n",
    "        border-radius: 15px;\n",
    "        text-align: center;\n",
    "        color: white;\n",
    "        font-size: 2rem;\n",
    "        font-weight: bold;\n",
    "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    .haram-result {\n",
    "        background: linear-gradient(135deg, #C73E1D 0%, #e74c3c 100%);\n",
    "        padding: 2rem;\n",
    "        border-radius: 15px;\n",
    "        text-align: center;\n",
    "        color: white;\n",
    "        font-size: 2rem;\n",
    "        font-weight: bold;\n",
    "        box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    .confidence-box {\n",
    "        background: #f8f9fa;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    .ingredient-box {\n",
    "        background: #e9ecef;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        margin: 0.5rem 0;\n",
    "        font-family: monospace;\n",
    "    }\n",
    "    .stButton>button {\n",
    "        width: 100%;\n",
    "        background: linear-gradient(135deg, #2E86AB 0%, #1a5f7a 100%);\n",
    "        color: white;\n",
    "        font-size: 1.2rem;\n",
    "        padding: 0.8rem;\n",
    "        border: none;\n",
    "        border-radius: 10px;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .stButton>button:hover {\n",
    "        background: linear-gradient(135deg, #1a5f7a 0%, #0d3b4d 100%);\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL AND TOKENIZER\n",
    "# ============================================================================\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load Keras or TFLite model\"\"\"\n",
    "    try:\n",
    "        if model_path.endswith('.h5'):\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            st.success(f\"‚úÖ Loaded Keras model: {model_path}\")\n",
    "            return model, 'keras'\n",
    "        elif model_path.endswith('.tflite'):\n",
    "            interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "            interpreter.allocate_tensors()\n",
    "            st.success(f\"‚úÖ Loaded TFLite model: {model_path}\")\n",
    "            return interpreter, 'tflite'\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "@st.cache_resource\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    \"\"\"Load tokenizer from pickle or JSON\"\"\"\n",
    "    try:\n",
    "        if tokenizer_path.endswith('.pkl'):\n",
    "            with open(tokenizer_path, 'rb') as f:\n",
    "                tokenizer = pickle.load(f)\n",
    "            st.success(f\"‚úÖ Loaded tokenizer from pickle\")\n",
    "            return tokenizer, 'pickle'\n",
    "        elif tokenizer_path.endswith('.json'):\n",
    "            with open(tokenizer_path, 'r', encoding='utf-8') as f:\n",
    "                tokenizer_json = json.load(f)\n",
    "            st.success(f\"‚úÖ Loaded tokenizer from JSON\")\n",
    "            return tokenizer_json, 'json'\n",
    "    except Exception as e:\n",
    "        st.error(f\"‚ùå Error loading tokenizer: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_text(text, tokenizer, tokenizer_type, max_length=256):\n",
    "    \"\"\"Preprocess text for model input\"\"\"\n",
    "    \n",
    "    if tokenizer_type == 'pickle':\n",
    "        # Keras tokenizer\n",
    "        sequences = tokenizer.texts_to_sequences([text])\n",
    "        padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, \n",
    "            maxlen=max_length, \n",
    "            padding='post', \n",
    "            truncating='post'\n",
    "        )\n",
    "        return padded\n",
    "    \n",
    "    elif tokenizer_type == 'json':\n",
    "        # Manual tokenization from JSON\n",
    "        word_index = tokenizer['word_index']\n",
    "        config = tokenizer['config']\n",
    "        \n",
    "        # Lowercase if needed\n",
    "        if config.get('lower', True):\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Split\n",
    "        words = text.split(config.get('split', ' '))\n",
    "        \n",
    "        # Map to indices\n",
    "        sequence = []\n",
    "        for word in words:\n",
    "            if word in word_index:\n",
    "                sequence.append(word_index[word])\n",
    "            elif config.get('oov_token') and config['oov_token'] in word_index:\n",
    "                sequence.append(word_index[config['oov_token']])\n",
    "        \n",
    "        # Pad\n",
    "        if len(sequence) < max_length:\n",
    "            sequence = sequence + [0] * (max_length - len(sequence))\n",
    "        else:\n",
    "            sequence = sequence[:max_length]\n",
    "        \n",
    "        return np.array([sequence], dtype=np.int32)\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICTION\n",
    "# ============================================================================\n",
    "\n",
    "def predict(text, model, model_type, tokenizer, tokenizer_type, max_length=256):\n",
    "    \"\"\"Make prediction\"\"\"\n",
    "    \n",
    "    # Preprocess\n",
    "    input_data = preprocess_text(text, tokenizer, tokenizer_type, max_length)\n",
    "    \n",
    "    # Predict\n",
    "    if model_type == 'keras':\n",
    "        prediction = model.predict(input_data, verbose=0)\n",
    "        probability = float(prediction[0][0])\n",
    "    \n",
    "    elif model_type == 'tflite':\n",
    "        # Get input/output details\n",
    "        input_details = model.get_input_details()\n",
    "        output_details = model.get_output_details()\n",
    "        \n",
    "        # Set input\n",
    "        model.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Run inference\n",
    "        model.invoke()\n",
    "        \n",
    "        # Get output\n",
    "        output = model.get_tensor(output_details[0]['index'])\n",
    "        probability = float(output[0][0])\n",
    "    \n",
    "    # Classify\n",
    "    is_haram = probability > 0.5\n",
    "    confidence = probability if is_haram else (1 - probability)\n",
    "    \n",
    "    return {\n",
    "        'is_haram': is_haram,\n",
    "        'probability': probability,\n",
    "        'confidence': confidence,\n",
    "        'label': 'HARAM' if is_haram else 'HALAL'\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN APP\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Header\n",
    "    st.markdown('<div class=\"main-header\">üçî Halal/Haram Food Classifier</div>', unsafe_allow_html=True)\n",
    "    st.markdown('<div class=\"sub-header\">Bilingual Deep Learning Model for Food Ingredient Classification</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    # Sidebar\n",
    "    with st.sidebar:\n",
    "        st.header(\"‚öôÔ∏è Configuration\")\n",
    "        \n",
    "        # Model selection\n",
    "        st.subheader(\"üì¶ Model\")\n",
    "        model_files = list(Path('models').glob('*.h5')) + list(Path('models').glob('*.tflite'))\n",
    "        \n",
    "        if not model_files:\n",
    "            st.error(\"‚ùå No models found in 'models/' directory!\")\n",
    "            st.info(\"Please add your .h5 or .tflite model files to the 'models/' folder\")\n",
    "            return\n",
    "        \n",
    "        model_file = st.selectbox(\n",
    "            \"Select Model\",\n",
    "            model_files,\n",
    "            format_func=lambda x: x.name\n",
    "        )\n",
    "        \n",
    "        # Tokenizer selection\n",
    "        st.subheader(\"üìù Tokenizer\")\n",
    "        tokenizer_files = list(Path('models').glob('*.pkl')) + list(Path('models').glob('tokenizer*.json'))\n",
    "        \n",
    "        if not tokenizer_files:\n",
    "            st.error(\"‚ùå No tokenizer found!\")\n",
    "            st.info(\"Please add tokenizer.pkl or tokenizer.json to the 'models/' folder\")\n",
    "            return\n",
    "        \n",
    "        tokenizer_file = st.selectbox(\n",
    "            \"Select Tokenizer\",\n",
    "            tokenizer_files,\n",
    "            format_func=lambda x: x.name\n",
    "        )\n",
    "        \n",
    "        # Max length\n",
    "        max_length = st.slider(\"Max Sequence Length\", 64, 512, 256, 64)\n",
    "        \n",
    "        # Load button\n",
    "        if st.button(\"üîÑ Load Model & Tokenizer\"):\n",
    "            st.session_state.model, st.session_state.model_type = load_model(str(model_file))\n",
    "            st.session_state.tokenizer, st.session_state.tokenizer_type = load_tokenizer(str(tokenizer_file))\n",
    "            st.session_state.max_length = max_length\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        \n",
    "        # About\n",
    "        st.subheader(\"‚ÑπÔ∏è About\")\n",
    "        st.info(\"\"\"\n",
    "        **How it works:**\n",
    "        1. Enter food ingredients\n",
    "        2. Model analyzes text\n",
    "        3. Get halal/haram prediction\n",
    "        \n",
    "        **Supports:**\n",
    "        - English & Indonesian\n",
    "        - Multiple ingredients\n",
    "        - Real-time prediction\n",
    "        \"\"\")\n",
    "        \n",
    "        # Stats\n",
    "        if 'prediction_count' in st.session_state:\n",
    "            st.metric(\"Predictions Made\", st.session_state.prediction_count)\n",
    "    \n",
    "    # Main content\n",
    "    if 'model' not in st.session_state or st.session_state.model is None:\n",
    "        st.warning(\"‚ö†Ô∏è Please load a model from the sidebar first!\")\n",
    "        \n",
    "        st.markdown(\"### üìö Quick Start Guide\")\n",
    "        col1, col2, col3 = st.columns(3)\n",
    "        \n",
    "        with col1:\n",
    "            st.markdown(\"#### 1Ô∏è‚É£ Prepare Files\")\n",
    "            st.code(\"\"\"\n",
    "models/\n",
    "  ‚îú‚îÄ‚îÄ your_model.h5\n",
    "  ‚îî‚îÄ‚îÄ tokenizer.pkl\n",
    "            \"\"\")\n",
    "        \n",
    "        with col2:\n",
    "            st.markdown(\"#### 2Ô∏è‚É£ Load Model\")\n",
    "            st.write(\"Select model and tokenizer from sidebar\")\n",
    "        \n",
    "        with col3:\n",
    "            st.markdown(\"#### 3Ô∏è‚É£ Start Testing\")\n",
    "            st.write(\"Enter ingredients and get predictions!\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Input methods\n",
    "    st.header(\"üìù Input Ingredients\")\n",
    "    \n",
    "    tab1, tab2, tab3 = st.tabs([\"‚úçÔ∏è Manual Input\", \"üìÑ Examples\", \"üî¢ Batch Testing\"])\n",
    "    \n",
    "    with tab1:\n",
    "        st.subheader(\"Enter Food Ingredients\")\n",
    "        \n",
    "        # Text input\n",
    "        user_input = st.text_area(\n",
    "            \"Ingredients (comma-separated):\",\n",
    "            placeholder=\"Example: chicken, salt, pepper, onion, garlic\\nContoh: ayam, garam, lada, bawang, bawang putih\",\n",
    "            height=150,\n",
    "            help=\"Enter ingredients in English or Indonesian, separated by commas\"\n",
    "        )\n",
    "        \n",
    "        # Language hint\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            st.caption(\"üá¨üáß English: chicken, beef, pork, wine, gelatin\")\n",
    "        with col2:\n",
    "            st.caption(\"üáÆüá© Indonesian: ayam, sapi, babi, arak, gelatin\")\n",
    "        \n",
    "        # Predict button\n",
    "        if st.button(\"üîÆ Predict\", type=\"primary\"):\n",
    "            if user_input.strip():\n",
    "                with st.spinner(\"Analyzing ingredients...\"):\n",
    "                    # Predict\n",
    "                    result = predict(\n",
    "                        user_input,\n",
    "                        st.session_state.model,\n",
    "                        st.session_state.model_type,\n",
    "                        st.session_state.tokenizer,\n",
    "                        st.session_state.tokenizer_type,\n",
    "                        st.session_state.max_length\n",
    "                    )\n",
    "                    \n",
    "                    # Update counter\n",
    "                    if 'prediction_count' not in st.session_state:\n",
    "                        st.session_state.prediction_count = 0\n",
    "                    st.session_state.prediction_count += 1\n",
    "                    \n",
    "                    # Display results\n",
    "                    st.markdown(\"---\")\n",
    "                    st.subheader(\"üéØ Prediction Results\")\n",
    "                    \n",
    "                    # Result box\n",
    "                    if result['is_haram']:\n",
    "                        st.markdown(f'<div class=\"haram-result\">‚ö†Ô∏è HARAM</div>', unsafe_allow_html=True)\n",
    "                    else:\n",
    "                        st.markdown(f'<div class=\"halal-result\">‚úÖ HALAL</div>', unsafe_allow_html=True)\n",
    "                    \n",
    "                    # Details\n",
    "                    col1, col2, col3 = st.columns(3)\n",
    "                    \n",
    "                    with col1:\n",
    "                        st.metric(\"Confidence\", f\"{result['confidence']*100:.2f}%\")\n",
    "                    \n",
    "                    with col2:\n",
    "                        st.metric(\"Probability Score\", f\"{result['probability']:.4f}\")\n",
    "                    \n",
    "                    with col3:\n",
    "                        status = \"üî¥ High Risk\" if result['is_haram'] else \"üü¢ Safe\"\n",
    "                        st.metric(\"Status\", status)\n",
    "                    \n",
    "                    # Ingredients display\n",
    "                    st.markdown(\"### üßæ Analyzed Ingredients:\")\n",
    "                    ingredients = [i.strip() for i in user_input.split(',')]\n",
    "                    \n",
    "                    cols = st.columns(3)\n",
    "                    for idx, ingredient in enumerate(ingredients):\n",
    "                        with cols[idx % 3]:\n",
    "                            st.markdown(f'<div class=\"ingredient-box\">‚Ä¢ {ingredient}</div>', unsafe_allow_html=True)\n",
    "                    \n",
    "                    # Interpretation\n",
    "                    st.markdown(\"### üí° Interpretation\")\n",
    "                    if result['is_haram']:\n",
    "                        st.error(\"\"\"\n",
    "                        **‚ö†Ô∏è This product may contain haram ingredients.**\n",
    "                        \n",
    "                        The model detected patterns associated with non-halal ingredients. \n",
    "                        Please verify the ingredient list carefully before consumption.\n",
    "                        \"\"\")\n",
    "                    else:\n",
    "                        st.success(\"\"\"\n",
    "                        **‚úÖ This product appears to be halal.**\n",
    "                        \n",
    "                        The model did not detect any haram ingredients. However, \n",
    "                        always check for halal certification for complete assurance.\n",
    "                        \"\"\")\n",
    "            else:\n",
    "                st.warning(\"‚ö†Ô∏è Please enter some ingredients first!\")\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"üìÑ Example Ingredients\")\n",
    "        \n",
    "        examples = {\n",
    "            \"‚úÖ Halal Example 1\": \"chicken breast, salt, black pepper, olive oil, garlic\",\n",
    "            \"‚úÖ Halal Example 2\": \"ayam, garam, lada hitam, minyak zaitun, bawang putih\",\n",
    "            \"‚úÖ Halal Example 3\": \"beef, potato, carrot, onion, tomato sauce\",\n",
    "            \"‚ö†Ô∏è Haram Example 1\": \"pork sausage, bacon, ham, lard\",\n",
    "            \"‚ö†Ô∏è Haram Example 2\": \"wine, beer, alcohol, rum extract\",\n",
    "            \"‚ö†Ô∏è Haram Example 3\": \"gelatin (pork), bacon bits, lard, pork fat\",\n",
    "        }\n",
    "        \n",
    "        for name, ingredients in examples.items():\n",
    "            with st.expander(name):\n",
    "                st.code(ingredients)\n",
    "                if st.button(f\"Test: {name}\", key=name):\n",
    "                    # Auto-fill and trigger prediction\n",
    "                    st.session_state.example_input = ingredients\n",
    "                    st.rerun()\n",
    "        \n",
    "        # Auto-fill from example\n",
    "        if 'example_input' in st.session_state:\n",
    "            st.info(f\"Example loaded: {st.session_state.example_input}\")\n",
    "            if st.button(\"üîÆ Predict Example\"):\n",
    "                result = predict(\n",
    "                    st.session_state.example_input,\n",
    "                    st.session_state.model,\n",
    "                    st.session_state.model_type,\n",
    "                    st.session_state.tokenizer,\n",
    "                    st.session_state.tokenizer_type,\n",
    "                    st.session_state.max_length\n",
    "                )\n",
    "                \n",
    "                if result['is_haram']:\n",
    "                    st.markdown(f'<div class=\"haram-result\">‚ö†Ô∏è HARAM</div>', unsafe_allow_html=True)\n",
    "                else:\n",
    "                    st.markdown(f'<div class=\"halal-result\">‚úÖ HALAL</div>', unsafe_allow_html=True)\n",
    "                \n",
    "                st.metric(\"Confidence\", f\"{result['confidence']*100:.2f}%\")\n",
    "                \n",
    "                del st.session_state.example_input\n",
    "    \n",
    "    with tab3:\n",
    "        st.subheader(\"üî¢ Batch Testing\")\n",
    "        \n",
    "        st.info(\"Upload a CSV file with 'ingredients' column for batch prediction\")\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\"Upload CSV\", type=['csv'])\n",
    "        \n",
    "        if uploaded_file:\n",
    "            import pandas as pd\n",
    "            \n",
    "            df = pd.read_csv(uploaded_file)\n",
    "            st.write(\"Preview:\", df.head())\n",
    "            \n",
    "            if 'ingredients' in df.columns:\n",
    "                if st.button(\"üöÄ Run Batch Prediction\"):\n",
    "                    with st.spinner(\"Processing batch...\"):\n",
    "                        results = []\n",
    "                        progress_bar = st.progress(0)\n",
    "                        \n",
    "                        for idx, row in df.iterrows():\n",
    "                            result = predict(\n",
    "                                row['ingredients'],\n",
    "                                st.session_state.model,\n",
    "                                st.session_state.model_type,\n",
    "                                st.session_state.tokenizer,\n",
    "                                st.session_state.tokenizer_type,\n",
    "                                st.session_state.max_length\n",
    "                            )\n",
    "                            results.append(result)\n",
    "                            progress_bar.progress((idx + 1) / len(df))\n",
    "                        \n",
    "                        df['prediction'] = [r['label'] for r in results]\n",
    "                        df['confidence'] = [r['confidence'] for r in results]\n",
    "                        \n",
    "                        st.success(\"‚úÖ Batch prediction complete!\")\n",
    "                        st.dataframe(df)\n",
    "                        \n",
    "                        # Download results\n",
    "                        csv = df.to_csv(index=False).encode('utf-8')\n",
    "                        st.download_button(\n",
    "                            \"üì• Download Results\",\n",
    "                            csv,\n",
    "                            \"batch_predictions.csv\",\n",
    "                            \"text/csv\"\n",
    "                        )\n",
    "            else:\n",
    "                st.error(\"CSV must have 'ingredients' column!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
